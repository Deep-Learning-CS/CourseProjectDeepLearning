# -*- coding: utf-8 -*-
"""model_development.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W63iBNVVhZey9potLAWv8qOOb6ocZoZc

# Define the Model
"""

# load librispeech dataset
!wget https://www.openslr.org/resources/12/dev-clean.tar.gz -P ./data/train/
!tar -xf ./data/train/dev-clean.tar.gz -C ./data/train/

import os

# define data directory
data_dir = './data/train/LibriSpeech/dev-clean/84/121123/'
higher_data_dir = './data/train/LibriSpeech/dev-clean/'
all_files = []

# load audio files into memory
for dir1 in os.listdir(higher_data_dir):
    for dir2 in os.listdir(higher_data_dir + dir1):
        for f in os.listdir(higher_data_dir + dir1 + '/' + dir2):
            file_path = higher_data_dir + dir1 + '/' + dir2 + '/' + f
            if f.endswith('.flac'):
                all_files.append(file_path)

print(len(all_files))

# Display a sample audio file
from IPython.display import Audio

display(Audio(all_files[0]))

"""# START AYUSH'S CODE"""

import librosa
import numpy as np
import matplotlib.pyplot as plt
import soundfile as sf
import librosa.display

def load_audio(file_path: str) -> np.ndarray:
    """
    Load an audio file and return its waveform as a numpy array.

    Parameters:
    file_path (str): Path to the audio file.

    Returns:
    np.ndarray: Audio waveform, and sample rate.
    """
    audio, sr = librosa.load(file_path, sr=16000)  # Load audio with original sampling rate
    return audio, sr

def extract_spectrogram(audio: np.ndarray, sr: int = 16000) -> np.ndarray:
    """
    Convert the audio waveform to a Mel spectrogram.

    Parameters:
    audio (np.ndarray): The audio waveform.
    sr (int): The sampling rate of the audio.

    Returns:
    np.ndarray: Spectrogram (magnitude in decibels).
    """
    # Extract Mel-spectrogram using librosa's correct method signature
    spectrogram = librosa.feature.melspectrogram(y=audio, sr=sr, n_fft=2048, hop_length=512)

    # Convert to decibels (dB scale)
    spectrogram_db = librosa.power_to_db(spectrogram, ref=np.max)
    # spectrogram_amp = librosa.db_to_amplitude(spectrogram)

    return spectrogram_db

def save_spectrogram(spectrogram: np.ndarray, file_path: str):
    """
    Save the spectrogram as a .npy file.

    Parameters:
    spectrogram (np.ndarray): The spectrogram to save.
    file_path (str): Path where to save the spectrogram file.
    """
    np.save(file_path, spectrogram)

def add_noise(audio: np.ndarray, noise_factor: float = 0.02) -> np.ndarray:
    """
    Adds random noise to an audio signal.

    Parameters:
    audio (np.ndarray): Input audio signal.
    noise_factor (float): Factor to scale the noise.

    Returns:
    np.ndarray: Noisy audio signal.
    """
    noise = np.random.randn(len(audio))
    noisy_audio = audio + noise_factor * noise

    # Clip the values to ensure they remain in the valid audio range (-1 to 1)
    noisy_audio = np.clip(noisy_audio, -1.0, 1.0)

    return noisy_audio

# Load clean audio
clean_audio, sr = load_audio(all_files[0])

# Add noise to the clean audio
noisy_audio = add_noise(clean_audio)

# Save the noisy audio as a new .flac file
sf.write('noisy_audio.flac', noisy_audio, sr, format='FLAC')

# Extract spectrogram for clean audio
clean_spectrogram = extract_spectrogram(clean_audio, sr)
clean_spectrogram.shape

# Extract spectrogram for noisy audio
noisy_spectrogram = extract_spectrogram(noisy_audio, sr)

# display clean audio
display(Audio(all_files[0]))

# display noisy audio
display(Audio('noisy_audio.flac'))

# Save the clean and noisy spectrograms
save_spectrogram(clean_spectrogram, "clean_spectrogram.npy")
save_spectrogram(noisy_spectrogram, "noisy_spectrogram.npy")

def plot_spectrogram(spectrogram, title):
    plt.figure(figsize=(10, 4))
    librosa.display.specshow(spectrogram, sr=sr, hop_length=512, x_axis='time', y_axis='mel')
    plt.colorbar(format='%+2.0f dB')
    plt.title(title)
    plt.tight_layout()
    plt.show()

# Plot clean and noisy spectrograms
plot_spectrogram(clean_spectrogram, 'Clean Audio Spectrogram')
plot_spectrogram(noisy_spectrogram, 'Noisy Audio Spectrogram')

"""# END AYUSH'S CODE"""

clean_spectrogram.shape

noisy_spectrogram.shape

# Librispeech is sampled at 16kHz
# since audio files are of different lengths, we need to standardize the lengths
# we can do this by separating audio into chunks
# I chose 16000 samples by 1.02 seconds, as the output shape for the spectrogram
# is then (128, 32)

samplerate = 16000
window_duration = 1.02
window_samples = int(samplerate * window_duration)

clean_spectrograms = []
noisy_spectrograms = []

# for each window in each audio file, get and store the associated spectrogram
for flac_file in all_files:
    for start in range(0, len(clean_audio), window_samples):
        end = start + window_samples

        clean_window = clean_audio[start:end]
        noisy_window = noisy_audio[start:end]

        if len(clean_window) < window_samples:
            continue

        clean_spectrogram = extract_spectrogram(clean_window, sr)
        clean_spectrograms.append(clean_spectrogram)

        noisy_spectrogram = extract_spectrogram(noisy_window, sr)
        noisy_spectrograms.append(noisy_spectrogram)

len(clean_spectrograms)

# turn list into numpy array
# since the model is expecting 3 channels, repeat the value 3 times
rgb_clean_spectrograms = np.repeat(np.array(clean_spectrograms)[..., np.newaxis], 3, axis=-1)
rgb_noisy_spectrograms = np.repeat(np.array(noisy_spectrograms)[..., np.newaxis], 3, axis=-1)

print(rgb_clean_spectrograms.shape)
print(rgb_noisy_spectrograms.shape)

# split train and test data
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(rgb_noisy_spectrograms, rgb_clean_spectrograms, test_size=0.1, random_state=42)

X_train.shape

# normalization so that each value in the spectrograms is within range of 0 and 1
min_val = -80
max_val = 0

scaled_X_train = (X_train - min_val) / (max_val - min_val)
scaled_X_test = (X_test - min_val) / (max_val - min_val)
scaled_y_train = (y_train - min_val) / (max_val - min_val)
scaled_y_test = (y_test - min_val) / (max_val - min_val)

print(np.min(scaled_X_train))
print(np.max(scaled_y_train))
print(np.min(scaled_X_test))
print(np.max(scaled_y_test))

plot_spectrogram(scaled_X_train[0], 'Noisy Audio Spectrogram')
plot_spectrogram(scaled_y_train[0], 'Clean Audio Spectrogram')

# function to turn the spectrogram back into the audio signal
def inverse_spectrogram(spectrogram):
    mel_spectrogram = librosa.db_to_power(spectrogram)
    linear_spectrogram = librosa.feature.inverse.mel_to_stft(mel_spectrogram, sr=16000, n_fft=2048)
    audio_signal = librosa.griffinlim(linear_spectrogram, n_iter=64, hop_length=512, n_fft=2048)
    audio_signal *= 50
    return audio_signal

# turn the first clean_spectrogram back into audio
inversed = inverse_spectrogram(clean_spectrograms[0])

# play the audio
display(Audio(inversed, rate=16000))

from tensorflow import keras
from keras.applications import VGG16, ResNet50

# Load a pretrained model and return the model.
def load_pretrained_model(model_name: str) -> keras.Model:
  if model_name == 'vgg':
    return VGG16(
        weights='imagenet',
        include_top=False,
        input_shape=(128, 32, 3)
    )
  elif model_name == 'resnet':
    return ResNet50(
        weights='imagenet',
        include_top=False,
        input_shape=(224, 224, 3)
    )
  else:
    return None

model = load_pretrained_model('vgg')

# Freeze the layers of a pretrained model to retain learned features.
# Return the model.
def freeze_layers(model: keras.Model) -> keras.Model:
  for layer in model.layers:
    layer.trainable = False
  return model

model = freeze_layers(model)
model.summary()

from keras.layers import Conv2D, UpSampling2D, BatchNormalization
from keras.models import Model

# Add custom layers to the pretrained model to turn it into an
# encoder-decoder architecture for audio denoising
def add_custom_layers(model: keras.Model) -> keras.Model:
  base_model = model

  # set the encoder to the pretrained model output
  encoder = model.output

  # Bottleneck - lower dimensional hidden layer where encoding is produced
  x = Conv2D(512, (3, 3), activation='relu', padding='same')(encoder)
  x = BatchNormalization()(x)

  def decoder_layers(x: keras.Model, unit_size: int) -> keras.Model:
    x = UpSampling2D((2, 2))(x)
    x = Conv2D(unit_size, (3, 3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    return x

  # Decoder - turn it back into a spectrogram
  x = decoder_layers(x, 256)
  x = decoder_layers(x, 128)
  x = decoder_layers(x, 64)
  x = decoder_layers(x, 32)
  x = decoder_layers(x, 16)

  outputs = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)

  return Model(inputs=base_model.input, outputs=outputs)

full_model = add_custom_layers(model)
full_model.summary()

# Compile the model using the given optimizer and loss function
def compile_model(loc_model: keras.Model, optimizer: str, loss_function: str):
  loc_model.compile(optimizer=optimizer, loss=loss_function, metrics=['mae'])

compile_model(full_model, 'rmsprop', 'mean_squared_error')

# Save the model to the given file path
def save_model(loc_model: keras.Model, file_path: str):
  loc_model.save(file_path)

save_model(full_model, "updated_model.keras")

full_model.fit(scaled_X_train, scaled_y_train, batch_size=64, epochs=50)

save_model(full_model, "trained_model.keras")

predictions = full_model.predict(scaled_X_test)

unscaled_predictions = predictions * (max_val - min_val) + min_val

test_inversed = inverse_spectrogram(np.squeeze(unscaled_predictions[0]))

plot_spectrogram(scaled_X_test[1], 'Noisy Audio Spectrogram')

plot_spectrogram(scaled_y_test[1], 'True Clean Spectrogram')

plot_spectrogram(np.squeeze(unscaled_predictions[1]), 'Clean Audio Spectrogram')

display(Audio(test_inversed, rate=16000))