# -*- coding: utf-8 -*-
"""model_development.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W63iBNVVhZey9potLAWv8qOOb6ocZoZc

# Define the Model
"""

# load librispeech dataset
!wget https://www.openslr.org/resources/12/dev-clean.tar.gz -P ./data/train/
!tar -xf ./data/train/dev-clean.tar.gz -C ./data/train/

import os

# define data directory
data_dir = './data/train/LibriSpeech/dev-clean/84/121123/'
all_files = os.listdir(data_dir)

# load audio files into memory
flac_files = [f for f in all_files if f.endswith('.flac')]
flac_files.sort()

print(flac_files)

# Display a sample audio file
from IPython.display import Audio

display(Audio(data_dir + flac_files[0]))

"""# START AYUSH'S CODE"""

import librosa
import numpy as np
import matplotlib.pyplot as plt
import soundfile as sf
import librosa.display

def load_audio(file_path: str) -> np.ndarray:
    """
    Load an audio file and return its waveform as a numpy array.

    Parameters:
    file_path (str): Path to the audio file.

    Returns:
    np.ndarray: Audio waveform, and sample rate.
    """
    audio, sr = librosa.load(file_path, sr=16000)  # Load audio with original sampling rate
    return audio, sr

def extract_spectrogram(audio: np.ndarray, sr: int = 16000) -> np.ndarray:
    """
    Convert the audio waveform to a Mel spectrogram.

    Parameters:
    audio (np.ndarray): The audio waveform.
    sr (int): The sampling rate of the audio.

    Returns:
    np.ndarray: Spectrogram (magnitude in decibels).
    """
    # Extract Mel-spectrogram using librosa's correct method signature
    spectrogram = librosa.feature.melspectrogram(y=audio, sr=sr, n_fft=2048, hop_length=512)

    # Convert to decibels (dB scale)
    spectrogram_db = librosa.power_to_db(spectrogram, ref=np.max)
    # spectrogram_amp = librosa.db_to_amplitude(spectrogram)

    return spectrogram_db

def save_spectrogram(spectrogram: np.ndarray, file_path: str):
    """
    Save the spectrogram as a .npy file.

    Parameters:
    spectrogram (np.ndarray): The spectrogram to save.
    file_path (str): Path where to save the spectrogram file.
    """
    np.save(file_path, spectrogram)

def add_noise(audio: np.ndarray, noise_factor: float = 0.02) -> np.ndarray:
    """
    Adds random noise to an audio signal.

    Parameters:
    audio (np.ndarray): Input audio signal.
    noise_factor (float): Factor to scale the noise.

    Returns:
    np.ndarray: Noisy audio signal.
    """
    noise = np.random.randn(len(audio))
    noisy_audio = audio + noise_factor * noise

    # Clip the values to ensure they remain in the valid audio range (-1 to 1)
    noisy_audio = np.clip(noisy_audio, -1.0, 1.0)

    return noisy_audio

# Load clean audio
clean_audio, sr = load_audio(data_dir + flac_files[0])

# Add noise to the clean audio
noisy_audio = add_noise(clean_audio)

# Save the noisy audio as a new .flac file
sf.write('noisy_audio.flac', noisy_audio, sr, format='FLAC')

# Extract spectrogram for clean audio
clean_spectrogram = extract_spectrogram(clean_audio, sr)
clean_spectrogram.shape

# Extract spectrogram for noisy audio
noisy_spectrogram = extract_spectrogram(noisy_audio, sr)

# display clean audio
display(Audio(data_dir + flac_files[0]))

# display noisy audio
display(Audio('noisy_audio.flac'))

# Save the clean and noisy spectrograms
save_spectrogram(clean_spectrogram, "clean_spectrogram.npy")
save_spectrogram(noisy_spectrogram, "noisy_spectrogram.npy")

def plot_spectrogram(spectrogram, title):
    plt.figure(figsize=(10, 4))
    librosa.display.specshow(spectrogram, sr=sr, hop_length=512, x_axis='time', y_axis='mel')
    plt.colorbar(format='%+2.0f dB')
    plt.title(title)
    plt.tight_layout()
    plt.show()

# Plot clean and noisy spectrograms
plot_spectrogram(clean_spectrogram, 'Clean Audio Spectrogram')
plot_spectrogram(noisy_spectrogram, 'Noisy Audio Spectrogram')

"""# END AYUSH'S CODE"""

clean_spectrogram.shape

noisy_spectrogram.shape

# Librispeech is sampled at 16kHz
# since audio files are of different lengths, we need to standardize the lengths
# we can do this by separating audio into chunks
# I chose 16000 samples by 1.02 seconds, as the output shape for the spectrogram
# is then (128, 32)

samplerate = 16000
window_duration = 1.02
window_samples = int(samplerate * window_duration)

clean_audios = []
clean_windows = []
clean_spectrograms = []
noisy_audios = []
noisy_windows = []
noisy_spectrograms = []

for flac_file in flac_files:
    # Load clean audios
    clean_audio, sr = load_audio(data_dir + flac_file)
    clean_audios.append((clean_audio, sr))

    # Add noise to the clean audios
    noisy_audio = add_noise(clean_audio)
    noisy_audios.append((noisy_audio, sr))

    # for each window in the audio file, get and store the associated spectrogram
    for start in range(0, len(clean_audio), window_samples):
        end = start + window_samples

        clean_window = clean_audio[start:end]
        noisy_window = noisy_audio[start:end]

        if len(clean_window) < window_samples:
            continue

        clean_windows.append(clean_window)
        noisy_windows.append(noisy_window)

        clean_spectrogram = extract_spectrogram(clean_window, sr)
        clean_spectrograms.append(clean_spectrogram)

        noisy_spectrogram = extract_spectrogram(noisy_window, sr)
        noisy_spectrograms.append(noisy_spectrogram)

# normalization so that each value in the spectrogram is within range of 0 and 1
min_val = np.min(clean_spectrograms)
max_val = np.max(clean_spectrograms)

scaled_clean_spectrograms = (clean_spectrograms - min_val) / (max_val - min_val)
scaled_noisy_spectrograms = (noisy_spectrograms - min_val) / (max_val - min_val)

# since the model is expecting 3 channels, repeat the value 3 times
np_clean_spectrograms = np.array(scaled_clean_spectrograms)
rgb_clean_spectrograms = np.repeat(np_clean_spectrograms[..., np.newaxis], 3, axis=-1)
np_noisy_spectrograms = np.array(scaled_noisy_spectrograms)
rgb_noisy_spectrograms = np.repeat(np_noisy_spectrograms[..., np.newaxis], 3, axis=-1)

print(rgb_clean_spectrograms.shape)
print(rgb_noisy_spectrograms.shape)

plot_spectrogram(clean_spectrograms[1], 'Clean Audio Spectrogram')
plot_spectrogram(noisy_spectrograms[1], 'Noisy Audio Spectrogram')

def inverse_spectrogram(spectrogram):
    mel_spectrogram = librosa.db_to_power(spectrogram)
    linear_spectrogram = librosa.feature.inverse.mel_to_stft(mel_spectrogram, sr=16000, n_fft=2048)
    audio_signal = librosa.griffinlim(linear_spectrogram, n_iter=64, hop_length=512, n_fft=2048)
    audio_signal *= 50
    return audio_signal

# turn the first clean_spectrogram back into audio
inversed = inverse_spectrogram(clean_spectrograms[1])

# play the audio
sf.write('inversed.flac', inversed, 16000, format='FLAC')

display(Audio(inversed, rate=16000))

inversed2 = inverse_spectrogram(clean_spectrograms[2])
sf.write('inversed2.flac', inversed2, 16000, format='FLAC')
display(Audio(inversed2, rate=16000))

from tensorflow import keras

from keras.applications import VGG16, ResNet50

# Load a pretrained model and return the model.
def load_pretrained_model(model_name: str) -> keras.Model:
  if model_name == 'vgg':
    return VGG16(
        weights='imagenet',
        include_top=False,
        input_shape=(128, 32, 3)
    )
  elif model_name == 'resnet':
    return ResNet50(
        weights='imagenet',
        include_top=False,
        input_shape=(224, 224, 3)
    )
  else:
    return None

model = load_pretrained_model('vgg')

# Freeze the layers of a pretrained model to retain learned features.
# Return the model.
def freeze_layers(model: keras.Model) -> keras.Model:
  for layer in model.layers:
    layer.trainable = False
  return model

model = freeze_layers(model)
model.summary()

from keras.layers import Conv2D, UpSampling2D, BatchNormalization
from keras.models import Model

# Add custom layers to the pretrained model to turn it into an
# encoder-decoder architecture for audio denoising
def add_custom_layers(model: keras.Model) -> keras.Model:
  base_model = model

  # set the encoder to the pretrained model output
  encoder = model.output

  # Bottleneck - lower dimensional hidden layer where encoding is produced
  x = Conv2D(512, (3, 3), activation='relu', padding='same')(encoder)
  x = BatchNormalization()(x)

  def decoder_layers(x: keras.Model, unit_size: int) -> keras.Model:
    x = UpSampling2D((2, 2))(x)
    x = Conv2D(unit_size, (3, 3), activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    return x

  # Decoder - turn it back into a spectrogram
  x = decoder_layers(x, 256)
  x = decoder_layers(x, 128)
  x = decoder_layers(x, 64)
  x = decoder_layers(x, 32)
  x = decoder_layers(x, 16)

  outputs = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)

  return Model(inputs=base_model.input, outputs=outputs)

full_model = add_custom_layers(model)
full_model.summary()

# Compile the model using the given optimizer and loss function
def compile_model(loc_model: keras.Model, optimizer: str, loss_function: str):
  loc_model.compile(optimizer=optimizer, loss=loss_function, metrics=['mae'])

compile_model(full_model, 'rmsprop', 'mean_squared_error')

# Save the model to the given file path
def save_model(loc_model: keras.Model, file_path: str):
  loc_model.save(file_path)

save_model(full_model, "updated_model.keras")

full_model.fit(rgb_noisy_spectrograms, rgb_clean_spectrograms, batch_size=64, epochs=10)

predictions = full_model.predict(rgb_noisy_spectrograms)

unscaled_predictions = predictions * (max_val - min_val) + min_val

test_inversed = inverse_spectrogram(np.squeeze(unscaled_predictions[2]))

plot_spectrogram(clean_spectrograms[2], 'Noisy Audio Spectrogram')

plot_spectrogram(np.squeeze(unscaled_predictions[0]), 'Clean Audio Spectrogram')

display(Audio(test_inversed, rate=16000))