# -*- coding: utf-8 -*-
"""segan_finetuned_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X1I_Z0sTkaCyM7wYlV3QsSFtwuJoa7C6
"""

!pip install demucs

import librosa
import numpy as np
import matplotlib.pyplot as plt
import soundfile as sf
import librosa.display
import os
import tarfile

# Path to the dataset tar.gz file
dataset_path = '/content/dev-clean.tar.gz'
# Directory to save extracted files and spectrograms
extracted_dir = '/content/extracted'

spectrogram_dir = '/content/mel_spectrograms'

# Create directories if they don't exist
os.makedirs(extracted_dir, exist_ok=True)
os.makedirs(spectrogram_dir, exist_ok=True)

# Path to the dataset tar.gz file
dataset_path = '/content/dev-clean.tar.gz'
# Directory to extract the files
extracted_dir = '/content/extracted'

# Create the directory if it doesn't exist
os.makedirs(extracted_dir, exist_ok=True)

# Extract the dataset
try:
    with tarfile.open(dataset_path, 'r:gz') as tar:
        tar.extractall(path=extracted_dir)
        print(f"Extraction completed successfully to {extracted_dir}")
except tarfile.ReadError:
    print("Error: Unable to read the tar file. It might be corrupted.")
except EOFError:
    print("Error: The file seems to be incomplete or corrupted.")

"""**LOADING DS AND CREATING SPECTROGRMS **"""

import os
import tarfile
import librosa
import librosa.display
import numpy as np
import matplotlib.pyplot as plt

# Path to the dataset tar.gz file
dataset_path = '/content/dev-clean.tar.gz'
# Directory to extract the files
extracted_dir = '/content/extracted'
# Directory to save spectrogram images
spectrogram_dir = '/content/mel_spectrograms'

# Create directories if they don't exist
os.makedirs(extracted_dir, exist_ok=True)
os.makedirs(spectrogram_dir, exist_ok=True)

# Step 1: Extract the dataset
try:
    with tarfile.open(dataset_path, 'r:gz') as tar:
        tar.extractall(path=extracted_dir)
        print(f"Extraction completed successfully to {extracted_dir}")
except tarfile.ReadError:
    print("Error: Unable to read the tar file. It might be corrupted.")
except EOFError:
    print("Error: The file seems to be incomplete or corrupted.")

# Step 2: Function to generate and save Mel spectrogram
def save_mel_spectrogram(audio_path, save_path, noise_factor=0.005):
    """
    Generate and save a Mel spectrogram as an image.
    Optionally augment the audio with noise.

    Parameters:
    audio_path (str): Path to the input audio file.
    save_path (str): Path to save the Mel spectrogram image.
    noise_factor (float): Factor controlling the amount of noise for augmentation.
    """
    try:
        # Load the audio file
        y, sr = librosa.load(audio_path, sr=16000)  # Using 16kHz sample rate

        # Augment the audio by adding noise
        noise = np.random.randn(len(y))
        augmented_audio = y + noise_factor * noise

        # Generate Mel spectrogram
        S = librosa.feature.melspectrogram(y=augmented_audio, sr=sr, n_mels=128, fmax=8000)
        S_dB = librosa.power_to_db(S, ref=np.max)

        # Save Mel spectrogram as image
        plt.figure(figsize=(10, 4))
        librosa.display.specshow(S_dB, sr=sr, x_axis='time', y_axis='mel')
        plt.colorbar(format='%+2.0f dB')
        plt.title('Mel spectrogram')
        plt.tight_layout()

        # Ensure the directory exists
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        plt.savefig(save_path)
        plt.close()
        print(f"Saved Mel spectrogram at {save_path}")
    except Exception as e:
        print(f"Error processing {audio_path}: {e}")

# Step 3: Process each .flac file in the extracted dataset
def process_dataset(dataset_folder, output_folder, noise_factor=0.005):
    """
    Process each .flac file in a dataset folder, generate Mel spectrograms, and save them as images.

    Parameters:
    dataset_folder (str): Path to the folder containing .flac files.
    output_folder (str): Path to save the spectrogram images.
    noise_factor (float): Factor controlling the amount of noise for augmentation.
    """
    for root, _, files in os.walk(dataset_folder):
        for file in files:
            if file.endswith(".flac"):
                # Input and output paths
                audio_path = os.path.join(root, file)
                relative_path = os.path.relpath(root, dataset_folder)
                spectrogram_save_path = os.path.join(output_folder, relative_path, f"{os.path.splitext(file)[0]}.png")

                # Process and save the spectrogram
                save_mel_spectrogram(audio_path, spectrogram_save_path, noise_factor)

# Example usage
process_dataset(extracted_dir, spectrogram_dir)

import librosa
import numpy as np
import soundfile as sf
import os

# Define directories for input and output
extracted_dir = '/content/extracted'  # The directory containing the raw audio files
noisy_audio_dir = '/content/noisy_audio'  # Directory to save noisy audio files
clean_audio_dir = '/content/clean_audio'  # Directory to save clean audio files

# Create directories if they don't exist
os.makedirs(noisy_audio_dir, exist_ok=True)
os.makedirs(clean_audio_dir, exist_ok=True)

# Function to generate and save noisy/clean audio
def save_noisy_and_clean_audio(audio_path, clean_save_path, noisy_save_path, noise_factor=0.005):
    try:
        # Load the clean audio file
        y, sr = librosa.load(audio_path, sr=16000)  # Using 16kHz sample rate

        # Save clean audio in .flac format
        sf.write(clean_save_path, y, sr)
        print(f"Saved clean audio at {clean_save_path}")

        # Augment the audio by adding noise
        noise = np.random.randn(len(y))  # White noise
        augmented_audio = y + noise_factor * noise  # Add noise to the clean audio

        # Save noisy audio in .flac format
        sf.write(noisy_save_path, augmented_audio, sr)
        print(f"Saved noisy audio at {noisy_save_path}")
    except Exception as e:
        print(f"Error processing {audio_path}: {e}")

# Loop through all audio files in the extracted directory and process them
found_files = 0  # To track if any files are found
for root, dirs, files in os.walk(extracted_dir):
    for file in files:
        if file.endswith(".flac"):  # Process only .flac files
            found_files += 1
            audio_file = os.path.join(root, file)

            # Define paths for saving clean and noisy audio
            clean_audio_path = os.path.join(clean_audio_dir, os.path.basename(file).replace('.flac', '_clean.flac'))
            noisy_audio_path = os.path.join(noisy_audio_dir, os.path.basename(file).replace('.flac', '_noisy.flac'))

            # Generate clean and noisy audio
            save_noisy_and_clean_audio(audio_file, clean_audio_path, noisy_audio_path)

if found_files == 0:
    print("No .flac files found in the extracted directory.")
else:
    print(f"Processed {found_files} .flac files.")

import librosa
import numpy as np
import soundfile as sf
import os

# Define directories for input and output
extracted_dir = '/content/extracted'  # The directory containing the raw audio files
noisy_audio_dir = '/content/noisy_audio'  # Directory to save noisy audio files
clean_audio_dir = '/content/clean_audio'  # Directory to save clean audio files

# Target length in samples (5 seconds at 16 kHz)
target_length = 16000 * 5  # 5 seconds of audio at 16kHz sample rate

# Create directories if they don't exist
os.makedirs(noisy_audio_dir, exist_ok=True)
os.makedirs(clean_audio_dir, exist_ok=True)

# Function to generate and save noisy/clean audio with standard size
def save_noisy_and_clean_audio(audio_path, clean_save_path, noisy_save_path, noise_factor=0.005):
    try:
        # Load the clean audio file
        y, sr = librosa.load(audio_path, sr=16000)  # Using 16kHz sample rate

        # Trim or pad the audio to the target length
        if len(y) > target_length:
            y = y[:target_length]  # Trim if longer
        elif len(y) < target_length:
            y = np.pad(y, (0, target_length - len(y)), mode='constant')  # Pad if shorter

        # Save clean audio in .flac format
        sf.write(clean_save_path, y, sr)
        print(f"Saved clean audio at {clean_save_path}")

        # Augment the audio by adding noise
        noise = np.random.randn(len(y))  # White noise
        augmented_audio = y + noise_factor * noise  # Add noise to the clean audio

        # Save noisy audio in .flac format
        sf.write(noisy_save_path, augmented_audio, sr)
        print(f"Saved noisy audio at {noisy_save_path}")
    except Exception as e:
        print(f"Error processing {audio_path}: {e}")

# Loop through all audio files in the extracted directory and process them
found_files = 0  # To track if any files are found
for root, dirs, files in os.walk(extracted_dir):
    for file in files:
        if file.endswith(".flac"):  # Process only .flac files
            found_files += 1
            audio_file = os.path.join(root, file)

            # Define paths for saving clean and noisy audio
            clean_audio_path = os.path.join(clean_audio_dir, os.path.basename(file).replace('.flac', '_clean.flac'))
            noisy_audio_path = os.path.join(noisy_audio_dir, os.path.basename(file).replace('.flac', '_noisy.flac'))

            # Generate clean and noisy audio with standard size
            save_noisy_and_clean_audio(audio_file, clean_audio_path, noisy_audio_path)

if found_files == 0:
    print("No .flac files found in the extracted directory.")
else:
    print(f"Processed {found_files} .flac files.")

!pip install speechbrain torchaudio soundfile



"""#SEGAN"""

!pip install torch torchaudio numpy scipy librosa matplotlib

import torch
import torchaudio
import librosa
import numpy as np
import soundfile as sf
from torch import nn
from torch.utils.data import DataLoader
import os
import matplotlib.pyplot as plt

from speechbrain.pretrained import SepformerSeparation as separator
import torch
import librosa
import soundfile as sf

# Load the pretrained SepFormer model
separator = separator.from_hparams(source="speechbrain/sepformer-wham-enhancement", savedir="tmpdir")

# Function to load and process the noisy audio file
def load_and_process_audio(audio_path):
    # Load the noisy audio
    y, sr = librosa.load(audio_path, sr=16000)  # Using 16kHz sample rate
    return y, sr

# Function to denoise the audio using SepFormer
def denoise_audio(model, noisy_audio_path):
    # Load noisy audio
    noisy_audio, sr = load_and_process_audio(noisy_audio_path)

    # Perform speech enhancement (denoising)
    # The input should be a tensor, so we convert it to a tensor
    noisy_audio_tensor = torch.tensor(noisy_audio).unsqueeze(0)  # Add batch dimension

    # Denoise using the pre-trained model (this returns the enhanced audio directly)
    enhanced_audio = model.separate_batch(noisy_audio_tensor)

    # Remove batch dimension and convert tensor back to numpy
    enhanced_audio = enhanced_audio.squeeze(0).detach().numpy()

    return enhanced_audio, sr

# Example usage
noisy_audio_path = "/content/noisy_audio/1272-128104-0000_noisy.flac"  # Path to the noisy audio file
denoised_audio, sr = denoise_audio(separator, noisy_audio_path)

# Save the denoised audio to a new file
sf.write("/content/denoised.flac", denoised_audio, sr)

print("Denoising complete! The result is saved as 'denoised_audio.wav'.")

import librosa
import librosa.display
import matplotlib.pyplot as plt

# Path to the noisy audio file
noisy_audio_path = "/content/noisy_audio/1272-128104-0000_noisy.flac"  # Update this with your file path

# Load the noisy audio file
y, sr = librosa.load(noisy_audio_path, sr=16000)

# Display the waveform
plt.figure(figsize=(12, 4))
librosa.display.waveshow(y, sr=sr)
plt.title("Waveform of Noisy Audio")
plt.xlabel("Time (s)")
plt.ylabel("Amplitude")
plt.tight_layout()
plt.show()
from IPython.display import Audio


# Play the audio file
Audio(noisy_audio_path)

from IPython.display import Audio

# Path to the denoised audio file
denoised_audio_path = "/content/denoised.flac"  # Update this path if necessary

# Play the denoised audio file
Audio(denoised_audio_path)

# If not already installed, you need to install these packages
!pip install torch torchaudio librosa
!pip install deepfilternet  # if using SEGAN from DeepFilterNet

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import librosa
import soundfile as sf
import os
import numpy as np

# Check if CUDA is available, otherwise use CPU
device = torch.device('cpu')

# Define the Generator and Discriminator models (Simplified)
class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()
        self.conv1 = nn.Conv1d(1, 32, kernel_size=15, stride=1, padding=7)
        self.conv2 = nn.Conv1d(32, 1, kernel_size=15, stride=1, padding=7)

    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = self.conv2(x)
        return x

class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.conv1 = nn.Conv1d(1, 32, kernel_size=15, stride=1, padding=7)
        self.fc1 = nn.Linear(32 * 16000, 1)

    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = x.view(x.size(0), -1)  # Flatten for fully connected layer
        x = self.fc1(x)
        return x

# Custom Dataset to load noisy and clean audio pairs
class AudioDataset(Dataset):
    def __init__(self, noisy_dir, clean_dir):
        self.noisy_dir = noisy_dir
        self.clean_dir = clean_dir
        self.noisy_files = [f for f in os.listdir(noisy_dir) if f.endswith(".flac")]
        self.clean_files = [f for f in os.listdir(clean_dir) if f.endswith(".flac")]

    def __len__(self):
        return len(self.noisy_files)

    def __getitem__(self, idx):
        noisy_path = os.path.join(self.noisy_dir, self.noisy_files[idx])
        clean_path = os.path.join(self.clean_dir, self.clean_files[idx])

        # Load audio files using librosa
        noisy_audio, sr = librosa.load(noisy_path, sr=16000)
        clean_audio, _ = librosa.load(clean_path, sr=16000)

        # Ensure the length matches for both noisy and clean audio
        max_len = 16000  # Standard length
        if len(noisy_audio) > max_len:
            noisy_audio = noisy_audio[:max_len]
        else:
            noisy_audio = np.pad(noisy_audio, (0, max_len - len(noisy_audio)), mode='constant')

        if len(clean_audio) > max_len:
            clean_audio = clean_audio[:max_len]
        else:
            clean_audio = np.pad(clean_audio, (0, max_len - len(clean_audio)), mode='constant')

        # Convert to torch tensors
        noisy_audio = torch.tensor(noisy_audio).float().unsqueeze(0)  # Add channel dimension
        clean_audio = torch.tensor(clean_audio).float().unsqueeze(0)

        return noisy_audio, clean_audio

# Hyperparameters
batch_size = 4
lr = 1e-4
epochs = 10
noisy_audio_dir = '/content/noisy_audio'  # Path to noisy audio
clean_audio_dir = '/content/clean_audio'  # Path to clean audio

# Create DataLoader
dataset = AudioDataset(noisy_audio_dir, clean_audio_dir)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Initialize the models
generator = Generator().to(device)
discriminator = Discriminator().to(device)



# Loss functions
criterion_g = nn.MSELoss()  # Generator loss (mean squared error for reconstruction)
criterion_d = nn.BCEWithLogitsLoss()  # Discriminator loss (binary cross-entropy)

# Optimizers
optimizer_g = optim.Adam(generator.parameters(), lr=lr)
optimizer_d = optim.Adam(discriminator.parameters(), lr=lr)

# Fine-Tuning Loop
for epoch in range(epochs):
    generator.train()
    discriminator.train()

    for noisy_audio, clean_audio in dataloader:
        noisy_audio = noisy_audio.to(device)
        clean_audio = clean_audio.to(device)

        # Train Discriminator
        optimizer_d.zero_grad()
        real_output = discriminator(clean_audio)
        fake_output = discriminator(generator(noisy_audio))

        # Real and Fake labels for the discriminator
        real_labels = torch.ones_like(real_output)
        fake_labels = torch.zeros_like(fake_output)

        loss_d_real = criterion_d(real_output, real_labels)
        loss_d_fake = criterion_d(fake_output, fake_labels)
        loss_d = (loss_d_real + loss_d_fake) / 2
        loss_d.backward()
        optimizer_d.step()

        # Train Generator
        optimizer_g.zero_grad()
        fake_output = discriminator(generator(noisy_audio))

        # Generator loss: Minimize the discriminator's ability to distinguish fake from real
        loss_g = criterion_g(generator(noisy_audio), clean_audio) + criterion_d(fake_output, real_labels)
        loss_g.backward()
        optimizer_g.step()

    print(f"Epoch [{epoch+1}/{epochs}], Loss G: {loss_g.item()}, Loss D: {loss_d.item()}")

# Save the fine-tuned model after training
torch.save(generator.state_dict(), '/content/generator_finetuned.pth')
torch.save(discriminator.state_dict(), '/content/discriminator_finetuned.pth')

# Inference: Use the fine-tuned generator to denoise a new audio file
generator.eval()
noisy_audio_path = '/content/noisy_audio/1272-128104-0000_noisy.flac'
noisy_audio, sr = librosa.load(noisy_audio_path, sr=16000)
noisy_audio_tensor = torch.tensor(noisy_audio).float().unsqueeze(0).unsqueeze(0).to(device)

with torch.no_grad():
    denoised_audio = generator(noisy_audio_tensor).squeeze().cpu().numpy()

# Save the denoised audio to a file
sf.write("/content/denoised_finetuned.flac", denoised_audio, sr)

print("Denoised audio saved to '/content/denoised_finetuned.flac'")

# Path to the denoised audio file
denoised_audio_path = "/content/denoised_finetuned.flac"  # Update this path if necessary

# Play the denoised audio file
Audio(denoised_audio_path)